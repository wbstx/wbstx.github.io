<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="Abstract这篇文章旨在使用unsupervised的方法解决真实数据缺少label的问题。对于街景图片的语义分割问题，往往缺少大量的labelled 的数据作为training data. 现有的一些方法可以从GTA V中提取大量automatically labelled的街景图片。然而这样的数据由于和现实图片有较大的差异，作为training data训练出的网络很难泛化到真实输">
<meta name="keywords" content="Paper Reading,Domain Adaptaion,ECCV 2018">
<meta property="og:type" content="article">
<meta property="og:title" content="论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training">
<meta property="og:url" content="http://yoursite.com/2019/04/05/Self-Training/index.html">
<meta property="og:site_name" content="Fevre Dream">
<meta property="og:description" content="Abstract这篇文章旨在使用unsupervised的方法解决真实数据缺少label的问题。对于街景图片的语义分割问题，往往缺少大量的labelled 的数据作为training data. 现有的一些方法可以从GTA V中提取大量automatically labelled的街景图片。然而这样的数据由于和现实图片有较大的差异，作为training data训练出的网络很难泛化到真实输">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2019/04/05/Self-Training/teaser.png">
<meta property="og:updated_time" content="2019-04-17T07:29:52.232Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training">
<meta name="twitter:description" content="Abstract这篇文章旨在使用unsupervised的方法解决真实数据缺少label的问题。对于街景图片的语义分割问题，往往缺少大量的labelled 的数据作为training data. 现有的一些方法可以从GTA V中提取大量automatically labelled的街景图片。然而这样的数据由于和现实图片有较大的差异，作为training data训练出的网络很难泛化到真实输">
<meta name="twitter:image" content="http://yoursite.com/2019/04/05/Self-Training/teaser.png">
    
    
        
          
              <link rel="shortcut icon" href="/images/ukiyo-e.png">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/ukiyo-e.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/ukiyo-e.png">
          
        
    
    <!-- title -->
    <title>论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss -->
    
    
</head>

<body class="max-width mx-auto px3 ltr">    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/album/">Album</a></li>
        
      </ul>
    </span>
    <br>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2019/04/11/scient-research-writing/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoursite.com/2019/04/05/Self-Training/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoursite.com/2019/04/05/Self-Training/&text=论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoursite.com/2019/04/05/Self-Training/&title=论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoursite.com/2019/04/05/Self-Training/&is_video=false&description=论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training&body=Check out this article: http://yoursite.com/2019/04/05/Self-Training/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoursite.com/2019/04/05/Self-Training/&title=论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoursite.com/2019/04/05/Self-Training/&title=论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoursite.com/2019/04/05/Self-Training/&title=论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoursite.com/2019/04/05/Self-Training/&title=论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoursite.com/2019/04/05/Self-Training/&name=论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Method"><span class="toc-number">2.</span> <span class="toc-text">Method</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Supervised-Domain-Adaptation"><span class="toc-number">2.1.</span> <span class="toc-text">Supervised Domain Adaptation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Unsupervised-Domain-Adaptation"><span class="toc-number">2.2.</span> <span class="toc-text">Unsupervised Domain Adaptation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Self-Training"><span class="toc-number">2.3.</span> <span class="toc-text">Self Training</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Class-balanced-Self-Training"><span class="toc-number">2.4.</span> <span class="toc-text">Class-balanced Self Training</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Self-paced-learning-policy-design"><span class="toc-number">2.5.</span> <span class="toc-text">Self-paced learning policy design</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Incorporating-spatial-priors"><span class="toc-number">2.6.</span> <span class="toc-text">Incorporating spatial priors</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Comments"><span class="toc-number">3.</span> <span class="toc-text">Comments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Code"><span class="toc-number">4.</span> <span class="toc-text">Code</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Fevre Dream</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2019-04-05T07:05:15.000Z" itemprop="datePublished">2019-04-05</time>
        
      
    </div>


      

      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/Domain-Adaptaion/">Domain Adaptaion</a>, <a class="tag-link" href="/tags/ECCV-2018/">ECCV 2018</a>, <a class="tag-link" href="/tags/Paper-Reading/">Paper Reading</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <center>
<img src="/2019/04/05/Self-Training/teaser.png" title="teaser">
</center>

<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>这篇<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yang_Zou_Unsupervised_Domain_Adaptation_ECCV_2018_paper.pdf" target="_blank" rel="noopener">文章</a>旨在使用unsupervised的方法解决真实数据缺少label的问题。对于街景图片的语义分割问题，往往缺少大量的labelled 的数据作为training data. 现有的一些方法可以从<a href="https://arxiv.org/pdf/1608.02192.pdf" target="_blank" rel="noopener">GTA V</a>中提取大量automatically labelled的街景图片。然而这样的数据由于和现实图片有较大的差异，作为training data训练出的网络很难泛化到真实输入上去。因此本文提出了一种unsupervised方法，先用 GTA V 数据预训练网络，再用没有label的真实数据来finetune网络来让网络可以应对真实情况。这就是domain adaptation的工作目标。</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>核心是一个交叉优化问题。</p>
<h3 id="Supervised-Domain-Adaptation"><a href="#Supervised-Domain-Adaptation" class="headerlink" title="Supervised Domain Adaptation"></a>Supervised Domain Adaptation</h3><p>在生成数据(source)和真实数据(target)都有Label的情况下，一般是用少量的target data来fineture用source data训练出来的网络，adaptation问题的loss一般是如下形式：</p>
<script type="math/tex; mode=display">\min_{w}\mathcal{L}_{S} (w)=-\sum^{S} _{s=1} \sum^{N} _{n=1} y^\intercal _{s,n} \log (p_n(w, I_s)) - \sum^{T} _{t=1} \sum^{N} _{n=1} y^\intercal _{t,n} \log (p_n(w, I_t))</script><p>其中\(I<em>s\)代表source input。\(y</em>{s,n}\)代表\(I_s\)第n个pixel的ground truth label. \(w\)代表网络权重，\(p_n(w, I_s)\)代表\(I_s\)的第n个像素通过网络的softmax输出结果。符号定义对于target是相似的。<br><strong>这个公式的核心就是降低预测结果和ground truth的交叉熵，是基本的语义分割的loss。</strong></p>
<h3 id="Unsupervised-Domain-Adaptation"><a href="#Unsupervised-Domain-Adaptation" class="headerlink" title="Unsupervised Domain Adaptation"></a>Unsupervised Domain Adaptation</h3><p>不同于supervised domain adaptation, target label是缺失的，因此可以考虑target label是一个隐变量，和网络权重一样是可以学习到的。这样思考的话上面的公式就变成了</p>
<script type="math/tex; mode=display">\min_{w, \hat{y}}\mathcal{L}_{U} (w, \hat{y})=-\sum^{S}_{s=1} \sum^{N} _{n=1} y^\intercal _{s,n} \log (p_n(w, I_s)) - \sum^{T} _{t=1} \sum^{N} _{n=1} \hat{y}^\intercal _{t,n} \log (p_n(w, I_t)) \\ \mathrm{ s.t. }\space\hat{y}_{t,n}\in{\{ e^i|e^i\in \mathcal{R}^C\}}​</script><p>其中\(\hat{y}\)称作pseudo-label, \(C\)是class的数量,\(e^i\)是one-hot向量。这样将target label也作为优化的对象，对此使用self-training的优化策略。</p>
<h3 id="Self-Training"><a href="#Self-Training" class="headerlink" title="Self Training"></a>Self Training</h3><p>按照前文所说，self-training实际上就是解上一个公式的优化问题，然而解出来的\(\hat{y}\)很难保证是正确的，所以self-trainging的过程采用了easy-to-hard模式，先找到confidence更高的target label用于更新网络，再逐渐采用confidence稍低的target label来逐步finetune，实际上是一个交叉优化问题。公式为</p>
<script type="math/tex; mode=display">\min_{w, \hat{y}}\mathcal{L}_{ST} (w, \hat{y})=-\sum^{S} _{s=1} \sum^{N} _{n=1} y^\intercal _{s,n} \log (p_n(w, I_s)) - \sum^{T} _{t=1} \sum^{N} _{n=1} [ \hat{y}^\intercal _{t,n} \log (p_n(w, I_t)) + k\lvert \hat{y}_{t,n} \rvert_1 ] \\

\mathrm{ s.t. }\space\hat{y}_{t,n}\in{\{ e^i|e^i\in \mathcal{R}^C\}}, k>0</script><p><strong>这个公式里比较核心的一点是引入了L1 norm。因为求交叉熵的关系，如果直接将pseudo label设为0就可以直接求得最小值0。引入L1 norm相当于为优化目标提供了负值走向，从而避免将pseudo label全设为0的结果。</strong></p>
<p>优化分为两个步骤</p>
<ol>
<li>固定\(w\)，优化\(\hat{y}_{t,n}\)</li>
<li>固定\(\hat{y}_{t,n}\)，优化\(w\)</li>
</ol>
<p>步骤1选出部分confidence较高的label，步骤2基于这些label来优化权重。步骤2的优化基于网络学习的随机梯度下降，而步骤1是非线性的整数规划问题。将对\(\hat{y}\)的优化单独拆分出来</p>
<script type="math/tex; mode=display">\min_{\hat{y}}- \sum^{T} _{t=1} \sum^{N} _{n=1} [ \sum^{C} _{c=1} \hat{y}^{(c)} _{t,n} \log (p_n(c|w, I_t)) + k\lvert \hat{y}_{t,n} \rvert_1 ] \\ \mathrm{ s.t. }\space\hat{y}_{t,n} = [ \hat{y}^{(1)} _{t,n}, \cdots , \hat{y}^{(C)} _{t,n} ]\in{ \{ \{ e^i|e^i\in \mathcal{R}^C\}} \cup 0 \}, k>0</script><p>\(\hat{y}_{t,n}\)可以通过以下算子优化</p>
<script type="math/tex; mode=display">\hat{y}^{(c)*} _{t,n}=\begin{cases}
1 & \text{if } \underset{c}{\text{argmax }}c=p_n(c|w, I_t),p_n(c|w, I_t) > exp(-k)  \\
0 & \text{otherwise}
\end{cases}</script><p>\(p_n(c|w, I_t) &gt; exp(-k)\)的限制条件可以保证得到负值输出，优于将pseudo label完全设为0的结果。</p>
<h3 id="Class-balanced-Self-Training"><a href="#Class-balanced-Self-Training" class="headerlink" title="Class-balanced Self Training"></a>Class-balanced Self Training</h3><p>对于多class分割，以上方法由于采用了easy-to-hard模式，会让网络更倾向于在易于分辨的class处表现良好而在不易分辨的class处效果很差（或者是某个在图片中占比很低的class）。因此需要做class-balance来保证每个class都能被很好分割。</p>
<script type="math/tex; mode=display">\min_{w, \hat{y}}\mathcal{L}_{ST} (w, \hat{y})=-\sum^{S} _{s=1} \sum^{N} _{n=1} y^\intercal _{s,n} \log (p_n(w, I_s)) - \sum^{T} _{t=1} \sum^{N} _{n=1} \sum^{C} _{c=1} [ \hat{y}^{(c)} _{t,n} \log (p_n(c|w, I_t)) + k_c\lvert \hat{y}^{(c)} _{t,n} \rvert_1 ] \\ \mathrm{ s.t. }\space\hat{y}_{t,n} = [ \hat{y}^{(1)} _{t,n}, \cdots , \hat{y}^{(C)} _{t,n} ]\in{ \{ \{ e^i|e^i\in \mathcal{R}^C\}} \cup 0 \}, k_c>0</script><p>其实改进很简单，不同的class设置不同的k值。k值越大，class所占权重也越大。</p>
<p>优化算子则变成了</p>
<script type="math/tex; mode=display">\hat{y}^{(c)*} _{t,n}=\begin{cases}
1 & \text{if } \underset{c}{\text{argmax }}c=\frac{p_n(c|w, I_t)}{exp(-k_c)},\frac{p_n(c|w, I_t)}{exp(-k_c)} > 1  \\
0 & \text{otherwise}
\end{cases}</script><p>通过normalization后，low score但是confidence很高的class得以被选出。因此\(k_c\)的选择也需要体现出这一点。</p>
<h3 id="Self-paced-learning-policy-design"><a href="#Self-paced-learning-policy-design" class="headerlink" title="Self-paced learning policy design"></a>Self-paced learning policy design</h3><p>由上面的内容可知，k值的选择在self learning的过程中至关重要。</p>
<p>选定k的流程为(伪代码)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">0</span>,T):</span><br><span class="line">  P_It = P(w, I_t)</span><br><span class="line">  MP_It = max(P_It, axis=<span class="number">0</span>)</span><br><span class="line">  M = [M, matrix_to_vector(MP_It)]</span><br><span class="line">M = sort(M, order=descending)</span><br><span class="line">length = len(M) * p</span><br><span class="line">k = -log(M[length])</span><br><span class="line"><span class="keyword">return</span> k</span><br></pre></td></tr></table></figure>
<p>这段伪代码的逻辑很简单。用现有的网络权重w为所有的target data做一次预测，然后对每个预测结果的confidence做一次排序， k的值则设为exp(-k) = p <em> T </em> N。p从20%开始逐渐增加到50%，每做一次交叉优化步骤，p增加5%。</p>
<p>也就是说，由于这种k的选择方式，对于生成的target label 有1-p的label是0，即无效label。在下一步进行对网络权重的优化时，只选择前p的confidence高的label作为实际有效label。逐步增加p从而达到<strong>easy-to-hard</strong>的模式。</p>
<p>对于class-balanced的$k_c$选择其实非常类似（伪代码）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">0</span>,T):</span><br><span class="line">  P_It = P(w, I_t)</span><br><span class="line">  LP_It = argmax(P, axis=<span class="number">0</span>)</span><br><span class="line">  MP_It = max(P_It, axis=<span class="number">0</span>)</span><br><span class="line">  <span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">0</span>, C):</span><br><span class="line">    MP_c_It = MP_It(LP_It==c)</span><br><span class="line">    M_c = [M_c, matrix_to_vector(MP_c_It)]</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> range(<span class="number">0</span>, C):</span><br><span class="line">	M_c = sort(M_c, order=descending)</span><br><span class="line">	length = len(M_c) * p</span><br><span class="line">	k_c = -log(M_c[length])</span><br><span class="line"><span class="keyword">return</span> k_c</span><br></pre></td></tr></table></figure>
<p>对所有被分类为c的pixel排序，$k_c$设为exp($-k_c$)=p * $N_c$, $N_c$为被分类为c的pixel总数。</p>
<h3 id="Incorporating-spatial-priors"><a href="#Incorporating-spatial-priors" class="headerlink" title="Incorporating spatial priors"></a>Incorporating spatial priors</h3><p>最后文章还介绍了基于一个大体分布的方式作为先验知识来提升效果的方法。这个要求data的分布比较接近，相对比较苛刻，所以不再详解了 。大概来说就是用Guassian Kernal处理ground truth统计不同class可能在图片上的分布位置。</p>
<h2 id="Comments"><a href="#Comments" class="headerlink" title="Comments"></a>Comments</h2><p>这篇文章核心思路其实就是一个交叉优化的过程，有几点比较值得借鉴的是</p>
<ol>
<li><p>easy-to-hard的优化过程</p>
</li>
<li><p>class-balance的引入</p>
</li>
</ol>
<p>文章也提到了domain transfer除了self-learning之外还有很多基于adversarial learning, style transfer的方法。想到之前看过的一篇文章用CycleGAN来用虚拟数据transfer成真实数据来解决真实数据难label的问题还感到惊为天人，其实不过是domain transfer领域的冰山一角而已。以后有机会或许会试着做那一篇文章的笔记。Domain transfer确实是一个很有意思的领域，以后也会读一些其他相关paper。 </p>
<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><div style="text-align:center">
  <div class="github-card" data-user="yzou2" data-repo="cbst" data-width="400" data-theme="default" data-target data-client-id data-client-secret></div>
</div>
<script src="/github-card-lib/githubcard.js"></script>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/album/">Album</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Method"><span class="toc-number">2.</span> <span class="toc-text">Method</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Supervised-Domain-Adaptation"><span class="toc-number">2.1.</span> <span class="toc-text">Supervised Domain Adaptation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Unsupervised-Domain-Adaptation"><span class="toc-number">2.2.</span> <span class="toc-text">Unsupervised Domain Adaptation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Self-Training"><span class="toc-number">2.3.</span> <span class="toc-text">Self Training</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Class-balanced-Self-Training"><span class="toc-number">2.4.</span> <span class="toc-text">Class-balanced Self Training</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Self-paced-learning-policy-design"><span class="toc-number">2.5.</span> <span class="toc-text">Self-paced learning policy design</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Incorporating-spatial-priors"><span class="toc-number">2.6.</span> <span class="toc-text">Incorporating spatial priors</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Comments"><span class="toc-number">3.</span> <span class="toc-text">Comments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Code"><span class="toc-number">4.</span> <span class="toc-text">Code</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoursite.com/2019/04/05/Self-Training/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoursite.com/2019/04/05/Self-Training/&text=论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoursite.com/2019/04/05/Self-Training/&title=论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoursite.com/2019/04/05/Self-Training/&is_video=false&description=论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training&body=Check out this article: http://yoursite.com/2019/04/05/Self-Training/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoursite.com/2019/04/05/Self-Training/&title=论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoursite.com/2019/04/05/Self-Training/&title=论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoursite.com/2019/04/05/Self-Training/&title=论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoursite.com/2019/04/05/Self-Training/&title=论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoursite.com/2019/04/05/Self-Training/&name=论文笔记-Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2019 TANG
  </div>
  <div class="footer-right">
    <!-- <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/album/">Album</a></li>
        
      </ul>
    </nav> -->
    Find me on
    
    
    
      
        <a class="icon" target="_blank" href="http://github.com/wbstx">
          <i class="fab fa-github"></i><!--
    ---></a><!--
  ---><!--
  --->.
      
    
  </div>
</footer>

    </div>
    <!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

    <!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Disqus Comments -->


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
</body>
</html>
